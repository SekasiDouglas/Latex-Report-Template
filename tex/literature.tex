\chapter{Literature Review}

\section{Introduction}
The main purpose of this chapter is to present some general consensus on the theoretical support and previous empirical studies on the various computer vision and object detection techniques. In this respect, the chapter provides an empirical platform for use in the investigation of which training techniques should be used in computer vision and which computer vision libraries are most appropriate.

\section{Computer Vision}
Computer vision is the science and technology of machines that see, where see in this case means that the machine is able to extract information from an image that is necessary to solve some task. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner \cite{forbes}.
As a technological discipline, computer vision seeks to apply its theories and models to the construction of computer vision systems. Examples of applications of computer vision include systems for:
\begin{itemize}
\item Controlling processes (e.g., an industrial robot or an autonomous vehicle).
\item Detecting events (e.g., for visual surveillance or people counting).
\item Organizing information (e.g., for indexing databases of images and image sequences).
\item Modeling objects or environments (e.g., industrial inspection, medical image analysis or topographical modeling).
\item Signal processing and pattern recognition.
\end{itemize}
Computer vision is closely related to the study of biological vision. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, learning, indexing, motion estimation, and image restoration.

\subsection{Fundamentals of Computer Vision}
Computer vision has experienced growth with its applications expanding in diverse fields: medical diagnostic imaging; factory automation; remote sensing; forensics; autonomous vehicle and robot guidance  \cite{landau}. Computer vision is the construction of explicit and meaningful descriptions of physical objects from images. The term which is synonymous with machine vision embodies several processes. 

Images are acquired with a physical image sensor and dedicated computing hardware and software are used to analyze the images with the objective of performing a predefined visual task. Machine vision is also recognized as the integrated use of devices for non-contact optical sensing and computing and decision processes to receive and interpret an image of a real scene automatically. The technology aims to duplicate the effect of human vision by electronically perceiving and understanding an image  \cite{articleX}. 

\section{Object Detection}
Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings or cars) in digital images and videos  \cite{bulgac}. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance. Examples of object detection tasks are face, eye and nose detection as well as logo detection.

Given an image, object detection is to determine whether or not the specified object is present, and, if present, determine the locations and sizes of each object. 
Kalinke also suggested that object detection and recognition mainly focuses on:-
Representation: How to represent an object.
Learning: Machine learning algorithms to learn the common property of a class of objects. 
Recognition: Identify the object in an image using models. 


\section{Review of Object Detection Techniques}
\subsection{Haartraining}
This is a technique used to train the boosted cascade object detector. It uses haar-like features and is used to train classifiers for image detection systems. OpenCV refers to this detector as the “Haar classifier” because it uses Haar features or, more precisely, Haar-like wavelets that consist of adding and subtracting rectangular image regions before thresholding the result. We also note that the training (createsamples (), haartraining ()) and detecting (cvHaarDetectObjects ()) code works well on any objects (not just faces) that are consistently textured and mostly rigid.

\subsubsection{Sample Creation}
For training, a training sample must be collected. There are two sample types i.e. Negative samples and Positive samples.
Negative samples are non-object images. They are taken from arbitrary images and must not contain object representation.
Positive samples are object images. A positive image contains the target object which you want machine to detect.

\subsubsection{Haar Like Features}
A recognition process can be much more efficient if it is based on the detection of features that encode some information about the class to be detected. This is a description of a case of Haar-like features that encode the existence of oriented contrasts between regions in the image [8]. A set of these features can be used to encode the contrasts exhibited by a human face and their special relationships. Haar-like features are so called because they are computed similar to the coefficients in Haar wavelet transforms. 

The object detector of OpenCV has been initially proposed and improved by Rainer Lienhart  \cite{articleY}. First, a classifier (namely a cascade of boosted classifiers working with haar-like features) is trained with a few hundreds of sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size. 

Performing haartraining for face detection insists that after a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. 

The classifier outputs a "1" if the region is likely to show the object (i.e., face/car), and "0" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily "resized" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales  \cite{turner}. 

\subsection{Knowledge and Motion Based Methods}
Mobile and Stationary Computer Vision based Traffic Surveillance Techniques for Advanced ITS Applications uses Knowledge-based methods that employ a priori knowledge to hypothesize vehicle locations in an image [13]. The useful information can be used includes symmetry, color, shadow, corners, horizontal/vertical edges, texture and vehicle lights. The Knowledge-based methods discussed above use spatial features to distinguish between vehicles and background. 

Another important cue that can be used is the relative motion obtained via the calculation of optical flow. Approaching vehicles at an opposite direction produce a diverging flow, which can be quantitatively distinguished from the flow caused by the car ego-motion. On the other hand, departing or passing vehicles produce a converging flow. In contrast to dense optical flow, “sparse optical flow” utilizes image features, such as corners, local minima and maxima, or “Color Blob”. Although it can only produce a sparse flow, feature based method can provide sufficient information for vehicle detection. 

The above methods also use the statistic model-based algorithms to learn the characteristics of the vehicle class from a set of training images which capture the variability in vehicle appearance. Usually, the variability of the non-vehicle class (background) is also modeled to improve performance.

\subsection{Background Subtraction Method}
Research on Computer Vision for the OU Fish Visualization describes this method as a generic algorithm based on motion tracking and is used to address the problem of detecting moving objects [14]. The first stage of the process takes an incoming frame and subtracts it from a background image. This is then thresholded and passed to a blob filter that keeps only shapes of a certain size. The result is a rough segmentation of fish (object) from the video image. As a final step, the segmentation mask is fed back in to the system to update the background. 

In other words, it is used as a motion mask for preventing foreground objects from being added to the background image. When a new frame comes in to the system, the motion mask is used to pull out only background pixels and those pixels are accumulated into the background image.
The motion mask feedback is not flawless however and in some cases can result in “motion mask creep”. This is where the motion mask prevents any new information from being added to the background making the background information more and more out of date. 

If the motion mask is identifying a fish, this is not a problem since identifying a fish is the goal, but if the motion mask is erroneously marking a location where some other type of movement is occurring such as water motion or changes in lighting, then the erroneous segmentation will actually be amplified as the segmented region grows.

Fortunately the fact that the fish are constantly moving can be used to prevent this kind of mask creep. If a pixel hasn’t been updated for a given period of time because it has been constantly blocked by the motion mask, the motion mask is overridden and the pixel is updated anyway. If the pixel the next new pixel at this location is still different enough (meaning it’s likely a fish), then it will still be detected as a motion pixel. When added to the system, this timeout feature vastly improved the segmentation results by reducing the number false positives.

\subsection{Template matching approach}
Template matching approach is widely used in image processing to localize and identify shapes in an image. In this approach, one looks for parts in an image which match a template (or model)  \cite{vilenkin}.This technique is majorly used in visual pattern recognition, where one compares the template function to the input image by maximizing the spatial cross-correlation or by minimizing a distance, that provides the matching rate.

The strategy of this approach is: for each possible position (in the image), each possible rotation, or each other geometric transformation of the template, compare each pixel’s neighborhood to this template. After computing the matching rate for each possibility, select the largest one that exceeds a predefined threshold. The biggest disadvantage to this technique is that it is a very expensive operation while dealing with big templates and/or large sets of images. 